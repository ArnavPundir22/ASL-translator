# ğŸ¤Ÿ ASL Translator â€“ Real-Time Sign Language Recognition

An end-to-end **American Sign Language (ASL) recognition system** built using **MediaPipe landmark detection** and **Deep Learning (LSTM)**.  
This project covers the full pipeline: **data collection â†’ model training â†’ real-time inference** using a webcam.

---

## ğŸ“Œ Project Overview

This project translates **ASL hand gestures into text** by extracting human landmarks instead of raw images, making the system faster, lightweight, and robust to lighting conditions.

### Currently Supported Signs
- `hello`
- `thanks`
- `iloveyou`

---

## ğŸ§  How It Works

1. Webcam captures live video frames  
2. MediaPipe extracts **pose, face, and hand landmarks**  
3. Each frame is converted into **1662 numerical features**  
4. A sequence of 30 frames is fed to an **LSTM neural network**  
5. The model predicts the corresponding ASL gesture in real time  

---

## ğŸ“ Project Structure

```
ASL-translator/
â”‚
â”œâ”€â”€ collect_data.py              # Collect landmark data using webcam
â”œâ”€â”€ train_model.py               # Train LSTM model
â”œâ”€â”€ realtime_test.py             # Real-time ASL prediction
â”œâ”€â”€ utils.py                     # MediaPipe detection & keypoint extraction
â”‚
â”œâ”€â”€ requirements.txt             # Project dependencies
â”‚
â”œâ”€â”€ hand_landmarker.task         # MediaPipe hand model
â”œâ”€â”€ face_landmarker.task         # MediaPipe face model
â”œâ”€â”€ pose_landmarker_lite.task    # MediaPipe pose model
â”‚
â”œâ”€â”€ MP_Data/                     # Auto-generated dataset directory
â””â”€â”€ README.md
```

---

## âœ¨ Key Features

- Multi-modal landmark extraction (Face + Pose + Both Hands)
- Automatic dataset generation
- LSTM-based temporal gesture learning
- Real-time webcam inference
- Easy to extend with new gestures

---

## ğŸ› ï¸ Tech Stack

- Python 3
- OpenCV
- MediaPipe Tasks
- NumPy
- TensorFlow / Keras
- Scikit-learn

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/ArnavPundir22/ASL-translator.git
cd ASL-translator
```

### 2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Usage

### Step 1: Collect Training Data
```bash
python collect_data.py
```
- Records 30 sequences per gesture  
- Each sequence contains 30 frames  
- Saves landmark data in `MP_Data/`

---

### Step 2: Train the Model
```bash
python train_model.py
```
- Trains an LSTM-based classifier  
- Saves the trained model as `action.h5`

---

### Step 3: Run Real-Time Prediction
```bash
python realtime_test.py
```
- Displays predicted gestures live on the screen  
- Press **`q`** to exit  

---

## ğŸ§¬ Landmark Feature Breakdown

Each frame contains **1662 features**:

| Component | Features |
|---------|----------|
| Pose (33 Ã— 4) | 132 |
| Face (468 Ã— 3) | 1404 |
| Left Hand (21 Ã— 3) | 63 |
| Right Hand (21 Ã— 3) | 63 |
| **Total** | **1662** |

---

## ğŸ§ª Model Architecture

```
Input: (30, 1662)

LSTM (64)
â†“
LSTM (128)
â†“
LSTM (64)
â†“
Dense (64)
â†“
Dense (32)
â†“
Softmax Output
```

---

## ğŸš€ Future Improvements

- Add more ASL gestures
- Sentence-level translation
- Text-to-speech output
- GUI-based interface
- Transformer-based sequence models

---

## âš ï¸ Notes

- Use consistent gestures during data collection
- Ensure good lighting
- Collect balanced samples for each class

---

## ğŸ‘¨â€ğŸ’» Author

**Arnav Pundir**  
Computer Vision & AI Developer  
GitHub: https://github.com/ArnavPundir22

---

â­ If you found this project useful, donâ€™t forget to star the repository!
